{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's recognise faces\n",
    "\n",
    "## Naive Face Verification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Packages needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have started by creating a virtual environment as per the readme.MD file. It eeds to be activated for this notebook!\n",
    "\n",
    "Can install needed packages in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some Jupyter housekeeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get an inception ResNet model trained with VGGface\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inception Model\n",
    "\n",
    "https://arxiv.org/pdf/1409.4842v1\n",
    "\n",
    "Neural network architecture codenamed Inception, which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14)\n",
    "\n",
    "![Why](meme.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two different kinds of inception blocks\n",
    "\n",
    "![module](module.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output size stays the same regardless of kernel size\n",
    "\n",
    "![Inception3D](inception3d.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which combine into GoogleNet\n",
    "\n",
    "![Summary](summary.png)\n",
    "\n",
    "![Layers](layers.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from facenet_pytorch import InceptionResnetV1, MTCNN, extract_face\n",
    "from torchview import draw_graph\n",
    "from sklearn.manifold import TSNE\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting a resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = InceptionResnetV1(pretrained=\"vggface2\", classify=True).cpu().eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_graph = draw_graph(\n",
    "    resnet, input_size=(1, 3, 256, 256), expand_nested=True\n",
    ")\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way PyTorch describes it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a guillotine to create `resnet_vector` through the `tweaked_resnet` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tweaked_resnet(nn.Module):\n",
    "    \"\"\"\n",
    "    A decapitated resnet version\n",
    "\n",
    "    The logits layer is removed from the resnet model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(tweaked_resnet, self).__init__()\n",
    "        self.resnet = InceptionResnetV1(\n",
    "            pretrained=\"vggface2\", classify=True\n",
    "        ).cpu()\n",
    "        self.resnet.logits = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_vector = tweaked_resnet()\n",
    "resnet_vector.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_graph = draw_graph(\n",
    "    resnet_vector,\n",
    "    input_size=(1, 3, 256, 256),\n",
    "    expand_nested=True,\n",
    ")\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A helper to transform, Multitask Cascaded Convolutional Networks for Face Detection and Alignment:\n",
    "\n",
    "MTCNN uses a cascade of three networks to detect faces and facial landmarks:\n",
    "\n",
    "* *PNet (Proposal Network)*: Scans the image and proposes candidate face regions.\n",
    "* *RNet (Refine Network)*: Refines the face proposals from PNet.\n",
    "* *ONet (Output Network)*: Detects facial landmarks (eyes, nose, mouth) and provides a final refinement of the bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcnn = MTCNN(select_largest=False)\n",
    "mtcnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using it as a helper but we are using the decapitated inception ResNet (`resnet_vector`) for the encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_encoding(image_path, model, transform):\n",
    "    \"\"\"\n",
    "    Convert any image into a 128-dimensional vector using the given model.\n",
    "    \"\"\"\n",
    "    img = Image.open(image_path)\n",
    "    img_t = transform(img)\n",
    "    batch_t = torch.unsqueeze(img_t, 0)\n",
    "    with torch.no_grad():\n",
    "        output = model(batch_t)  # here we get the 512-dimensional vector\n",
    "    return (output, image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a set of images in a folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER = \"images\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start encoding a picture of Jason Chan:\n",
    "\n",
    "<img src=\"images/chan1.png\" style=\"width:250px;height:250px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAN1 = FOLDER + \"/chan1.png\"\n",
    "\n",
    "jason_chan = img_to_encoding(CHAN1, resnet_vector, mtcnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise what happened:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pil = torchvision.transforms.ToPILImage()\n",
    "invert = torchvision.transforms.functional.invert\n",
    "\n",
    "mtcnn2 = MTCNN(select_largest=False, post_process=False)\n",
    "tensor = mtcnn2(Image.open(CHAN1))\n",
    "img = invert(to_pil(tensor))\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image above is after the transform (MTCNN), and below we get the vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jason_chan[0].shape)\n",
    "print(jason_chan[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's encode a group of people!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = {}\n",
    "database[\"Sammy Sum\"] = img_to_encoding(\n",
    "    FOLDER + \"/sum1.png\", resnet_vector, mtcnn\n",
    ")\n",
    "database[\"Jason Chan\"] = img_to_encoding(\n",
    "    FOLDER + \"/chan1.png\", resnet_vector, mtcnn\n",
    ")\n",
    "database[\"Alex Fong\"] = img_to_encoding(\n",
    "    FOLDER + \"/fong1.png\", resnet_vector, mtcnn\n",
    ")\n",
    "database[\"Dada Chan\"] = img_to_encoding(\n",
    "    FOLDER + \"/dada1.png\", resnet_vector, mtcnn\n",
    ")\n",
    "database[\"Niki Chow\"] = img_to_encoding(\n",
    "    FOLDER + \"/niki1.png\", resnet_vector, mtcnn\n",
    ")\n",
    "database[\"Shiga Lin\"] = img_to_encoding(\n",
    "    FOLDER + \"/shiga1.png\", resnet_vector, mtcnn\n",
    ")\n",
    "database[\"Gillian Chung\"] = img_to_encoding(\n",
    "    FOLDER + \"/gillian1.png\", resnet_vector, mtcnn\n",
    ")\n",
    "database[\"Charlene Choi\"] = img_to_encoding(\n",
    "    image_path=FOLDER + \"/choi1.png\", model=resnet_vector, transform=mtcnn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small helper here to visualise images in a folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images_in_folder(\n",
    "    folder, transformed=True, sigourney=True, last_number=\"\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Show all images in a folder.\n",
    "    Can filter Sigourney Weaver images.\n",
    "    Returns a list of vectors and image filenames.\n",
    "    \"\"\"\n",
    "    image_files = [\n",
    "        f\n",
    "        for f in sorted(os.listdir(folder))\n",
    "        if f.endswith((last_number + \".png\"))\n",
    "    ]\n",
    "    if not sigourney:\n",
    "        image_files = [f for f in image_files if not f.startswith(\"sig\")]\n",
    "\n",
    "    images_per_row = 6\n",
    "    _, axes = plt.subplots(\n",
    "        math.ceil(len(image_files) / images_per_row),\n",
    "        images_per_row,\n",
    "        figsize=(7, 4),\n",
    "    )\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    files_list, vectors_list = [], []\n",
    "    for idx, image_file in enumerate(image_files):\n",
    "        img_path = os.path.join(folder, image_file)\n",
    "        if transformed:\n",
    "            img = invert(to_pil(mtcnn2(Image.open(img_path))))\n",
    "        else:\n",
    "            img = Image.open(img_path)\n",
    "\n",
    "        axes[idx].imshow(img)\n",
    "        axes[idx].set_title(image_file)\n",
    "        axes[idx].axis(\"off\")\n",
    "        vec, _ = img_to_encoding(img_path, resnet_vector, mtcnn)\n",
    "        files_list.append(image_file)\n",
    "        vectors_list.append(vec[0])\n",
    "\n",
    "    for i in range(len(image_files), len(axes)):\n",
    "        axes[i].axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return np.array(vectors_list), files_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The selection contained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = show_images_in_folder(\n",
    "    FOLDER, transformed=False, sigourney=False, last_number=\"1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To quickly see what we have in the database now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = show_images_in_folder(FOLDER, sigourney=False, last_number=\"1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems we are ready to start recognising and verifying faces!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The HK boys: identity verification\n",
    "\n",
    "We are comparing the vector database entry with a specific vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ba2f317e79e15a2f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def verify(image_path, identity, database, model, transform):\n",
    "    \"\"\"\n",
    "    Function that verifies if the person on the \"image_path\" image is \"identity\".\n",
    "    \"\"\"\n",
    "    encoding = img_to_encoding(image_path, model, transform)[0]\n",
    "\n",
    "    # This is where we get the cosine similarity between the two vectors\n",
    "    # Cosine similarity is working with angles, not magnitudes, and we get a value between -1 and 1\n",
    "    # We want to get a dissimilarity value, so we take 1 - cosine similarity\n",
    "    dist = 1 - F.cosine_similarity(database[identity][0], encoding)\n",
    "\n",
    "    if dist < 0.5:\n",
    "        print(\"It's \" + str(identity) + \", welcome in!\")\n",
    "        door_open = True\n",
    "    else:\n",
    "        print(\"It's not \" + str(identity) + \", please go away\")\n",
    "        door_open = False\n",
    "\n",
    "    return dist, door_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def side_by_side(database_img, input_img):\n",
    "    \"\"\"\n",
    "    Shows image and recognised image\n",
    "    \"\"\"\n",
    "    to_pil = torchvision.transforms.ToPILImage()\n",
    "    invert = torchvision.transforms.functional.invert\n",
    "    mtcnn2 = MTCNN(select_largest=False, post_process=False)\n",
    "\n",
    "    img1 = invert(to_pil(mtcnn2(Image.open(database_img))))\n",
    "    img2 = invert(to_pil(mtcnn2(Image.open(input_img))))\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(4, 2))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    axes[0].axis(\"off\")\n",
    "    axes[0].imshow(img1)\n",
    "    axes[0].set_title(\"Database\")\n",
    "    axes[1].axis(\"off\")\n",
    "    axes[1].imshow(img2)\n",
    "    axes[1].set_title(\"Input\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jason Chan is now trying to enter the office, this is how he looks like today:\n",
    "\n",
    "<img src=FOLDER + \"/chan2.png\" style=\"width:250px;height:250px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-014d077254ad7d52",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "CHAN2 = FOLDER + \"/chan2.png\"\n",
    "\n",
    "distance, door_open_flag = verify(\n",
    "    CHAN2, \"Jason Chan\", database, resnet_vector, mtcnn\n",
    ")\n",
    "print(\"(\", distance, \",\", door_open_flag, \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did the system compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "side_by_side(database[\"Jason Chan\"][1], CHAN2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alex Fong is now trying to enter the office, but with Jason Chan's card... he's coming from the swimming pool today:\n",
    "\n",
    "<img src=\"images/fong2.png\" style=\"width:250px;height:250px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FONG2 = FOLDER + \"/fong2.png\"\n",
    "\n",
    "distance, door_open_flag = verify(\n",
    "    FONG2, \"Jason Chan\", database, resnet_vector, mtcnn\n",
    ")\n",
    "print(\"(\", distance, \",\", door_open_flag, \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did the system compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "side_by_side(database[\"Jason Chan\"][1], FONG2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But he decides to put his real identity, still out of the pool!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "distance, door_open_flag = verify(\n",
    "    FONG2, \"Alex Fong\", database, resnet_vector, mtcnn\n",
    ")\n",
    "print(\"(\", distance, \",\", door_open_flag, \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did the system compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "side_by_side(database[\"Alex Fong\"][1], FONG2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then Sammy Sum shows up, happily:\n",
    "\n",
    "<img src=\"images/sum2.png\" style=\"width:250px;height:250px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUM2 = FOLDER + \"/sum2.png\"\n",
    "\n",
    "distance, door_open_flag = verify(\n",
    "    SUM2, \"Sammy Sum\", database, resnet_vector, mtcnn\n",
    ")\n",
    "print(\"(\", distance, \",\", door_open_flag, \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did the system compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "side_by_side(database[\"Sammy Sum\"][1], SUM2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## The HK girls: recognising a face\n",
    "\n",
    "Now we are getting the closest entry in the vector database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a04ff2b5fd1186f8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def who_is_it(image_path, database, model, transform):\n",
    "    \"\"\"\n",
    "    Implements face recognition\n",
    "    \"\"\"\n",
    "    encoding = img_to_encoding(image_path, model, transform)[0]\n",
    "    min_dist = 100\n",
    "    for name, (db_enc, _) in database.items():\n",
    "        dist = 1 - F.cosine_similarity(db_enc, encoding)\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            identity = name\n",
    "\n",
    "    if min_dist > 0.7:\n",
    "        print(\"Not in the database.\")\n",
    "    else:\n",
    "        print(\"it's \" + str(identity) + \", the distance is \" + str(min_dist))\n",
    "    return min_dist, identity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dada Chan is now trying to enter the office:\n",
    "\n",
    "<img src=\"images/dada2.png\" style=\"width:250px;height:250px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DADA2 = FOLDER + \"/dada2.png\"\n",
    "\n",
    "_, who = who_is_it(DADA2, database, resnet_vector, mtcnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "side_by_side(database[who][1], DADA2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gillian Chung is now trying to enter the office:\n",
    "\n",
    "<img src=\"images/gillian2.png\" style=\"width:250px;height:250px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GILLIAN2 = FOLDER + \"/gillian2.png\"\n",
    "\n",
    "_, who = who_is_it(GILLIAN2, database, resnet_vector, mtcnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "side_by_side(database[who][1], GILLIAN2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And Shiga Lin\n",
    "\n",
    "<img src=\"images/shiga2.png\" style=\"width:250px;height:250px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHIGA2 = FOLDER + \"/shiga2.png\"\n",
    "\n",
    "_, who = who_is_it(SHIGA2, database, resnet_vector, mtcnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "side_by_side(database[who][1], SHIGA2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering the images\n",
    "\n",
    "Naughty Sigourney Weaver has added herself to the HK team... maybe not?\n",
    "\n",
    "The full folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors, files = show_images_in_folder(FOLDER, transformed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors, files = show_images_in_folder(FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "PCA finds the directions of maximum variance in the data and projects the data onto those directions. \n",
    "\n",
    "This way we can see which vectors are close or far\n",
    "\n",
    "Note that this is distance, which is NOT cosine similarity, but there is some correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d(vectors_2d, files_list):\n",
    "    \"\"\"\n",
    "    2D scatter plot of the images based on their position in a compact space\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.scatter(vectors_2d[:, 0], vectors_2d[:, 1])\n",
    "\n",
    "    for vec, image_file in zip(vectors_2d, files_list):\n",
    "        img = invert(\n",
    "            to_pil(mtcnn2(Image.open(os.path.join(FOLDER, image_file))))\n",
    "        )\n",
    "        ab = AnnotationBbox(\n",
    "            offsetbox=OffsetImage(img, zoom=0.2),\n",
    "            xy=vec,\n",
    "            frameon=False,\n",
    "            box_alignment=(0, 0),\n",
    "        )\n",
    "        ax.add_artist(ab)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.tensor(vectors)\n",
    "_, _, V = torch.pca_lowrank(tensor, q=2)\n",
    "tensor_pca = tensor @ V[:, :2]\n",
    "\n",
    "vectors.shape, tensor_pca.numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d(tensor_pca, files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE Dimensionality Reduction\n",
    "\n",
    "t-distributed Stochastic Neighbor Embedding\n",
    "\n",
    "* Converts Euclidean distances between points into probabilities using a Gaussian distribution in high-dimensional space.\n",
    "* In low-dimensional space, it uses a Student's t-distribution to compute similarities, which helps mitigate the \"crowding problem\" by allowing more flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_embedded = TSNE(\n",
    "    n_components=2, learning_rate=\"auto\", init=\"random\", perplexity=3\n",
    ").fit_transform(vectors)\n",
    "\n",
    "vectors_embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d(vectors_embedded, files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's all folks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TDF_face",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
